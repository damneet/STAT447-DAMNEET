---
output:
  pdf_document: default
  html_document: default
---
# Modelling Unemployment and Vaccination Rates
Damneet Thiara, 11170388 - STAT 447C

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressPackageStartupMessages(require(rstan))
suppressPackageStartupMessages(require(ggplot2))
suppressPackageStartupMessages(require(dplyr))
suppressPackageStartupMessages(require(bayesplot))
library(knitr)
```

```{r,echo=FALSE}
## Reading CSV files
vaccines<-read.csv("~/Desktop/vaccination-coverage-map_data.csv")
ur_data<-read.csv("~/Desktop/STAT 447C/PROJECT DATA/UR_DATA.csv")

## Isolating proportion at least 1 dose

Canadian_vaccines<-subset(vaccines, prename == "Canada")
Canadian_vaccines<-data.frame(
  date=Canadian_vaccines$week_end,
  dose_rate=Canadian_vaccines$proptotal_atleast1dose)

## Removing extra data

ur_data<-ur_data[-c(40,41),]
ur_data<-ur_data[-c(30:39),]

Canadian_vaccines<-Canadian_vaccines[-c(1,3:6,8:10,12:14,16:18,20:23,
                                        25:27,29:32,34:36,38:40,42:45,
                                        47:49,51:53,55:58,60:62,64:66,68:69,
                                        71,80,85:88),]

## Normalizing data

ur<-ur_data$UR/100
v<-Canadian_vaccines$dose_rate/100
N<-length(v)
df<-data.frame(Unemployment=ur,Vaccination=v)
```
## Introduction
By April of 2020, the overall unemployment rate in Canada had hit 14.1%, an 8.4 percentage point increase from February of 2020 (Source 1 - StatsCan). This unprecedented increase was just one month after COVID-19 was declared a pandemic by the World Health Organization (WHO). Since then, deaths have been unprecedented, alongside a dramatic downturn in economic activity. By December 9th, 2020, Health Canada had authorized the first COVID-19 vaccine: the Pfizer-BioNTech vaccine (Source 3). Health Canada also reported that the  Pfizer-BioNTech Comirnaty COVID vaccine had shown to have been “95% effective in protecting trial participants from COVID-19 for those 16 years and older” (Source 4). 

With consumer demand and economic activity spiralling downwards amidst a succession of COVID-19 lockdowns and increasing case/death counts, it begs the question on how COVID-19 vaccines have affected these economic characteristics. Specifically, this paper aims to analyze the relationship between vaccination rates and unemployment rates in Canada from 2020 to 2023. We test the difference between a logarithmic Frequentist model versus a Bayesian model to predict Canadian unemployment rates based on vaccination rates, using time series data.

## Literature Review
Previous studies on employing a Bayesian model to unemployment rates have been conducted by Source 5. By building a hierarchical Bayesian model, Source 5 uses cross-sectional and time series data to estimate US state unemployment rates. Additionally, Source 7 uses a Bayesian analysis to estimate the effects of COVID-19 on key economic indicators, and finds that unemployment increased, interest rates decreased, and prices fell after the first lockdown measures. These effects were then dampened or counteracted by stimulus packages. The increase in unemployment rates here ties in directly with the purpose of this project.

Source 8 conducts a Bayesian analysis on how vaccine misinformation affects vaccination rates, reporting that misinformation can decline intent to vaccinate by up to 6.4 percentage points. 

Finally, researchers have explored the relationship between vaccination rates and unemployment rates in many ways already. Source 9 uses an OLS model to find the relationship between vaccination rates and unemployment rates in the US. When analyzing domestic unemployment rates, they find a negative relationship between the two; an increase in vaccination rates is seen alongside a decrease in unemployment rates. However, when analyzing through a state-by-state view, they find that states with higher vaccination rates are more likely to have higher unemployment rates. Although this may seem counter-intuitive, the authors explain this by showing how states with higher vaccination rates experience a higher change in unemployment rates, indeed highlighting the benefits that higher vaccination rates have when it comes to unemplpyment. Indeed, source 3 also finds the same correlation between states with high unemployment rates and high vaccination rates. Similarly, Source 2 finds that there is also a negative relationship between veccination and unemployment rates, specifically using instrumental variable analysis to isolate the effects of vaccines by using pharamacy desnity as a proxy.

The causal relationship can also be reversed. It is shown that a 5 percentage point increase in unemployment rate, a characteristic increase during a recessionary period, decreases the likelihood of a flu shot by 2.3 percentage points (Source 4). 

Notably, there is little to no literature on comparing a Frequentist and Bayesian methods to estimate the relationship between vaccination rates and unemployment rates. 

## Analysis
### Data
Based on the literature review, although higher vaccination rates are associated with greater drops in unemployment, this does not necessarily mean that states or provinces with higher vaccination rates will have lower unemployment rates. Since the purpose of this project is to test the differences between a Frequentist and Bayesian model, for simplicity, national time series data for Canadian unemployment rates and vaccination rates is used, rather than dividing it between multiple provinces. 

The data for vaccination rates was retrieved from the Government of Canada Health Information Base (Source 6). Previous literature uses one dosage of a COVID-19 vaccine as the benchmark for vaccination rates, so the same standard is used here. Canadian unemployment rates were retrieved from Statistics Canada databases (Source 7). Unemployment rates are reported for an entire month, so the corresponding vaccination rate for each month is the last available vaccination rate available for the end of each month. Some months in 2023 are omitted due to limited vaccination rate data during this time. The final data looks like:

```{r,echo=FALSE,out.width="50%"}
plot(v,ur,
     main="Vaccination and Unemployment Rates in Canada, 2020 to 2023",
     ylab="Unemployment Rate",
     xlab="Vaccination Rate",
     ylim=c(0.045,0.1))
```

As vaccination rates go up, the unemployment rate that same month goes down.

### Frequentist Model
To begin, we start by building a logarithmic Ordinary Least Squares model:
$$log(U)=\beta_1V+\beta_0$$
The fitted model is as follows:
```{r,echo=FALSE}
ordinary_model<-lm(log(ur)~v)
summary(ordinary_model)
```
### Bayesian Model
Considering the restrictions on unemployment and vaccination rates, being between 0 and 1, a Beta proportion distribution is appropriate. The Beta proportion regression model can be defined as:
$$ U\sim Beta Proportion(\mu,\sigma)$$
$$\mu\in(0,1)$$
$$\sigma\in\mathbb{R}^+$$
$$U\in(0,1)$$
We define $\mu$ using an inverse logit function, which takes all real values as inputs and values between 0 and 1 as the output.
$$\mu=inv.logit(\beta_1V+\beta_0)$$
The first set of priors for the $\beta_1$ slope, $\beta_0$ intercept, and $\sigma$ parameters are as follows:
$$\beta_1\sim Normal(0,10)$$
$$\beta_0\sim Exp(0.1)$$
$$\sigma\sim Exp(0.01)$$

The slope parameter is normally distributed to allow for the possibility between positive and negative values. The intercept must be positive since we assume unemployment cannot be below 0, and $\sigma$ must also be positive, hence the exponential distribution.

The preliminary model produces the following results:
```{r,echo=FALSE}
slope_data<-c(2.69,0.01,0.32,-3.29,-2.90,-2.69,-2.48,-2.06,1046,1.00)
intercept_data<-c(0.07,0.00,0.07,0.00,0.02,0.04,0.10,0.24,1118,1.00)
sigma_data<-c(6.15,0.05,1.75,3.25,4.93,5.96,7.18,10.13,1137,1.00)

colnames<-c("mean","se_mean","sd","2.5%","25%","50%",
            "75%","97.5%","n_eff","Rhat")

fit1_table<-data.frame("slope"=slope_data,
                       "intercept"=intercept_data,
                       "sigma"=sigma_data)

fit1_table<-data.frame(t(fit1_table))
colnames(fit1_table)<-colnames
kable(fit1_table)
```
```{stan, echo=FALSE,output.var="MODEL1"}
data {
  int<lower=0> N; 
  vector<lower=0,upper=1>[N] v;
  vector<lower=0, upper=1>[N] u; 
  real<lower=0,upper=1> v_pred;
}


parameters {
  real slope;
  real<lower=0> intercept;
  real<lower=0> sigma;
}

transformed parameters {
  vector[N] mu=inv_logit(intercept + slope*v);
}



model {
  slope ~ normal(0,10);
  intercept ~ exponential(0.1);
  sigma ~ exponential(0.01);
  u~beta_proportion(mu, sigma);
  
}

generated quantities {
  real u_pred = beta_proportion_rng(inv_logit(intercept + slope*v_pred),sigma);
}

```
```{r, message=FALSE,echo=FALSE, warning=FALSE, results=FALSE, dependson=knitr::dep_prev()}
fit1 = sampling(
  seed=123,
  MODEL1,         
  data = list(u=ur,v=v,N=N,v_pred=1),      
  chains = 4,
  iter = 1000
)
```
```{r,echo=FALSE,out.width="50%"}
mu_values<-extract(fit1)$mu
averages <- colMeans(mu_values)

plot(v,averages,col="red",type="b",
      main="Bayesian Model 1",
      ylab="Unemployment Rate",
      xlab="Vaccine Rate")
```

The unemployment rates are clearly wrong, since the observed data for unemployment falls between 5% and 9%. The issue likely seems to be with the fact that unemployment rate does not vary much and remains at low levels, typically below 10%. Employing a beta prior on this raw data might not be appropriate, since the beta distribution ranges between 0 and 1. In order to combat, the unemployment rate is scaled upwards by a random constant. In this case, scaling upwards by 10 seems appropriate since none of the data for this model falls above 10%, so scaling by 10 will ensure that the data remains within the range of 0 to 1. After doing this, the following results are produced:
```{r,echo=FALSE}
ur=ur_data$UR/10
```
```{r,echo=FALSE, message=FALSE, warning=FALSE, results=FALSE, dependson=knitr::dep_prev()}
fit2 = sampling(
  seed=123,
  MODEL1,         
  data = list(u=ur,v=v,N=N,v_pred=1),      
  chains = 4,
  iter = 1000
)

mu_values<-extract(fit2)$mu/10
averages <- colMeans(mu_values)

```
```{r,echo=FALSE}
slope_data2<-c(-2.36,0.01,0.30,-2.99,-2.54,-2.35,-2.15,-1.79,743,1.00)
intercept_data2<-c(2.21,0.01,0.23,1.78,2.05,2.21,2.36,2.70,737,1.00)
sigma_data2<-c(45.59,0.38,11.47,25.87,37.59,44.66,52.40,70.90,893,1.01)
fit2_table<-data.frame("slope"=slope_data2,
                       "intercept"=intercept_data2,
                       "sigma"=sigma_data2)

fit2_table<-data.frame(t(fit2_table))
colnames(fit2_table)<-colnames
kable(fit2_table)
```

The key observations here are the intercept and the negative slope. The histograms for both are shown below.
```{r,echo=FALSE}
ur=ur_data$UR/10
```
```{r, echo=FALSE,message=FALSE, warning=FALSE, results=FALSE, dependson=knitr::dep_prev()}

fit2 = sampling(
  seed=123,
  MODEL1,         
  data = list(u=ur,v=v,N=N,v_pred=1),      
  chains = 4,
  iter = 1000
)

```
```{r,echo=FALSE,out.width="50%"}
slopes<-extract(fit2)$slope/10
intercepts<-extract(fit2)$intercept

hist(slopes,main="Histogram of Slopes")

inv_logit <- function(x) {
  exp_x <- exp(-x)
  return(1 / (1 + exp_x))
}

hist(inv_logit(intercepts)/10,main="Histogram of Intercepts - Model 2",
     xlab="Intercept")
```

To test different priors, I change the model priors to the following:
$$\beta_1\sim Normal(0,1)$$
$$\beta_0\sim Exp(0.01)$$
$$\sigma\sim Exp(0.1)$$
```{r,echo=FALSE}
slope_data3<-c(-2.09,0.02,0.33,-2.74,-2.31,-2.08,-1.86,-1.45,460,1.00)
intercept_data3<-c(2.02,0.01,0.25,1.54,1.84,2.01,2.19,2.51,473,1.00)
sigma_data3<-c(34.09,0.31,9.29,18.06,27.55,33.06,40.02,54.46,881,1.00)
fit3_table<-data.frame("slope"=slope_data3,
                       "intercept"=intercept_data3,
                       "sigma"=sigma_data3)

fit3_table<-data.frame(t(fit3_table))
colnames(fit3_table)<-colnames
kable(fit3_table)
```

The posterior estimates are slightly different but around the same range. Notably, the effective sample size has decreased in this new model, so the first model will be kept, considering the posterior estimates are only slighlty different.

## Results
### Frequentist vs Bayesian Model

The model data for both the Frequentist model and the chosen Bayesian model are plotted against the actual data below:

```{r,echo=FALSE,out.width="40%"}
dosage<-data.frame(v)

predictions<-predict(ordinary_model,newdata=dosage)

plot(v,ur/10,
     main="Vaccination and Unemployment Rates in Canada, 2020 to 2023",
     ylab="Unemployment Rate",
     xlab="Vaccination Rate",
     ylim=c(0.045,0.1),type="b",col="black")
lines(dosage$v,exp(predictions),type="b",col="red")
lines(v,averages,type="b",col="blue")
legend("bottomleft",
       legend=c("Actual", "Frequentist", "Bayesian"),
       col=c("black", "red", "blue"),
       pch = 1,
       bty="n",
       cex=0.8)
```

Both look similar, but the root mean squared error can be computed to compare the fit of both:
```{r,echo=FALSE}
errors_regular<-exp(predictions)-ur/10
RMSE1<-sqrt(mean(errors_regular^2))

errors_bayes1<-averages-ur/10

RMSE2<-sqrt(mean(errors_bayes1^2))

errors_table<-data.frame("Frequentist"=RMSE1,"Bayesian"=RMSE2)
kable(errors_table)
```

The RMSE for the Bayesian method is smaller, and thus fits better.

Finally, credible intervals and generated quantity predictions can be used to if the Bayesian model can predict the next missing data point. For each interval, we leave the ith observation out:
```{r,echo=FALSE,message=FALSE, warning=FALSE, results=FALSE, dependson=knitr::dep_prev()}
df<-data.frame(u=ur,v=v)


N_obs = nrow(df)
N_train = N_obs-1

ci_limits <- matrix(NA, nrow(df), 2)

for (i in 1:nrow(df)) {
  N_train <- nrow(df) - 1
  train_test_dta <- list(
    N = N_train,
    v = df$v[-i], 
    u = df$u[-i], 
    v_pred = df$v[i]
  )
  
  fit2 = sampling(
  seed=123,
  MODEL1,         
  data = train_test_dta,      
  chains = 4,
  iter = 1000
)
  
  samples <- (rstan::extract(fit2)$u_pred)
  
  obs_credible_interval <- quantile(samples, c(0.025, 0.975))
  
  ci_limits[i, ] <- obs_credible_interval
}
```
```{r,echo=FALSE,out.width="50%"}
merged_df = df %>% 
  bind_cols(data.frame(CI_L = ci_limits[,1], CI_R = ci_limits[,2])) %>% 
  mutate(Inside_CI = (u >= CI_L & u <= CI_R)) 
merged_df %>% 
  ggplot(aes(x = 1:N_obs, y = u, ymin = CI_L, ymax = CI_R, color=Inside_CI)) +
  geom_point() + 
  geom_errorbar() +
  theme_minimal() +
  labs(x = "Point", y = "Unemployment Rate")
```