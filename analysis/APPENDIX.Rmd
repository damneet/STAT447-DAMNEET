---
title: "Appendix"
author: "Damneet Thiara (11170388)"
date: "2024-04-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressPackageStartupMessages(require(rstan))
suppressPackageStartupMessages(require(ggplot2))
suppressPackageStartupMessages(require(dplyr))
suppressPackageStartupMessages(require(bayesplot))
```

## Cleaning Data

```{r}
## Reading CSV files
vaccines<-read.csv("~/Desktop/vaccination-coverage-map_data.csv")
ur_data<-read.csv("~/Desktop/STAT 447C/PROJECT DATA/UR_DATA.csv")

## Isolating proportion at least 1 dose

Canadian_vaccines<-subset(vaccines, prename == "Canada")
Canadian_vaccines<-data.frame(
  date=Canadian_vaccines$week_end,
  dose_rate=Canadian_vaccines$proptotal_atleast1dose)

## Removing extra data

ur_data<-ur_data[-c(40,41),]
ur_data<-ur_data[-c(30:39),]

Canadian_vaccines<-Canadian_vaccines[-c(1,3:6,8:10,12:14,16:18,20:23,
                                        25:27,29:32,34:36,38:40,42:45,
                                        47:49,51:53,55:58,60:62,64:66,68:69,
                                        71,80,85:88),]

## Normalizing data

ur<-ur_data$UR/100
v<-Canadian_vaccines$dose_rate/100
N<-length(v)
df<-data.frame(Unemployment=ur,Vaccination=v)
```


## Table of Data

```{r}

rmarkdown::paged_table(df)

```

```{r}
## Plot for first look at relationship between two variable

plot(v,ur,
     main="Vaccination and Unemployment Rates in Canada, 2020 to 2023",
     ylab="Unemployment Rate",
     xlab="Vaccination Rate",
     ylim=c(0.045,0.1))

```

Plot 1

## Fitting Frequentist Model

```{r}
## Logarithmimc model

ordinary_model<-lm(log(ur)~v)
summary(ordinary_model)
```

Summary statistics for frequentist model.

```{r}
## Measuring accuracy

dosage<-data.frame(v)

predictions<-predict(ordinary_model,newdata=dosage)

plot(v,ur,
     main="Vaccination and Unemployment Rates in Canada, 2020 to 2023",
     ylab="Unemployment Rate",
     xlab="Vaccination Rate",
     ylim=c(0.045,0.1))
lines(dosage$v,exp(predictions),type="p",col="red")
legend("bottomleft", legend = c("Actual", "Fitted"), col = c("black", "red"), pch = c(1, 1))

errors_regular<-exp(predictions)-ur
sqrt(mean(errors_regular^2))
```
Plot 2.
Square root of the mean squared error of residuals for this model is 0.007736867.

## Bayesian Model

### Model 1

I use the following priors:
$$U\sim Beta(\mu,\sigma)$$
$$\mu=inv.logit(\beta_1V+\beta_0)$$
$$\beta_1\sim Normal(0,10)$$
$$\beta_0\sim Exp(0.1)$$
$$\sigma\sim Exp(0.01)$$

```{stan, output.var="MODEL1"}
data {
  int<lower=0> N; 
  vector<lower=0,upper=1>[N] v;
  vector<lower=0, upper=1>[N] u; 
  real<lower=0,upper=1> v_pred;
}


parameters {
  real slope;
  real<lower=0> intercept;
  real<lower=0> sigma;
}

transformed parameters {
  vector[N] mu=inv_logit(intercept + slope*v);
}



model {
  slope ~ normal(0,10);
  intercept ~ exponential(0.1);
  sigma ~ exponential(0.01);
  u~beta_proportion(mu, sigma);
  
}

generated quantities {
  real u_pred = beta_proportion_rng(inv_logit(intercept + slope*v_pred),sigma);
}

```

### Fitting Model

```{r message=FALSE, warning=FALSE, results=FALSE, dependson=knitr::dep_prev()}
fit1 = sampling(
  seed=123,
  MODEL1,         
  data = list(u=ur,v=v,N=N,v_pred=1),      
  chains = 4,
  iter = 1000
)
```

```{r}
print(fit1)
```

```{r}
## Plot model

mu_values<-extract(fit1)$mu
averages <- colMeans(mu_values)

plot(v,averages,col="red",type="b",
      main="Bayesian Model 1",
      ylab="Unemployment Rate",
      xlab="Vaccine Rate")

```

Plot 3.

### Scaling Unemployment by 10

```{r}
ur=ur_data$UR/10
```

```{r message=FALSE, warning=FALSE, results=FALSE, dependson=knitr::dep_prev()}

fit2 = sampling(
  seed=123,
  MODEL1,         
  data = list(u=ur,v=v,N=N,v_pred=1),      
  chains = 4,
  iter = 1000
)

```

```{r}
print(fit2)
```

```{r}

## Plotting model
mu_values<-extract(fit2)$mu/10
averages <- colMeans(mu_values)

plot(v,averages,type="b",col="red",
     main="Bayesian Model 2 - Actual vs Predicted",
     ylim=c(0.045,0.095),
     xlab="Vaccine Rate",
     ylab="Unemployment Rate")
lines(v,ur/10,type="b")
legend("bottomleft", legend = c("Actual", "Predicted"), col = c("black", "red"), pch = c(1, 1))

## Plotting slope and intercept histograms

slopes<-extract(fit2)$slope/10
intercepts<-extract(fit2)$intercept

hist(slopes,main="Histogram of Slopes")

inv_logit <- function(x) {
  exp_x <- exp(-x)
  return(1 / (1 + exp_x))
}

hist(inv_logit(intercepts)/10,main="Histogram of Intercepts - Model 2",
     xlab="Intercept")
```

Plot 4, 5, and 6.

### Errors

```{r}
errors_bayes1 <- data.frame(matrix(ncol = 29, nrow = 2000))
  
errors_bayes1<-averages-ur/10

sqrt(mean(errors_bayes1^2))

```

Square root of the mean squared error for this model is 0.006672511.

### Model 2: Changing Priors

I use the following priors now:

$$U\sim Beta(\mu,\sigma)$$
$$\mu=inv.logit(\beta_1V+\beta_0)$$
$$\beta_1\sim Normal(0,1)$$
$$\beta_0\sim Exp(0.01)$$
$$\sigma\sim Exp(0.1)$$

```{stan, output.var="MODEL2"}
data {
  int<lower=0> N; 
  vector<lower=0,upper=1>[N] v;
  vector<lower=0, upper=1>[N] u; 
  real<lower=0,upper=1> v_pred;
}


parameters {
  real slope;
  real<lower=0> intercept;
  real<lower=0> sigma;
}

transformed parameters {
  vector[N] mu=inv_logit(intercept + slope*v);
}



model {
  slope ~ normal(0,1);
  intercept ~ exponential(0.01);
  sigma ~ exponential(0.1);
  u~beta_proportion(mu, sigma);
  
}

generated quantities {
  real u_pred = beta_proportion_rng(inv_logit(intercept + slope*v_pred),sigma);
}
```


```{r message=FALSE, warning=FALSE, results=FALSE, dependson=knitr::dep_prev()}

fit3 = sampling(
  seed=123,
  MODEL2,         
  data = list(u=ur,v=v,N=N,v_pred=1),      
  chains = 4,
  iter = 1000
)

```

```{r}
print(fit3)
```

### Errors

```{r}
mu_values<-extract(fit3)$mu/10
averages <- colMeans(mu_values)

errors_bayes2<-averages-ur/10

sqrt(mean(errors_bayes2^2))

```

This is higher than the square root mean squared error we had before, which was 0.006672511.

### Credible Intervals

```{r message=FALSE, warning=FALSE, results=FALSE, dependson=knitr::dep_prev()}
df<-data.frame(u=ur,v=v)


N_obs = nrow(df)
N_train = N_obs-1

ci_limits <- matrix(NA, nrow(df), 2)

for (i in 1:nrow(df)) {
  N_train <- nrow(df) - 1
  train_test_dta <- list(
    N = N_train,
    v = df$v[-i], 
    u = df$u[-i], 
    v_pred = df$v[i]
  )
  
  fit2 = sampling(
  seed=123,
  MODEL1,         
  data = train_test_dta,      
  chains = 4,
  iter = 1000
)
  
  samples <- (rstan::extract(fit2)$u_pred)
  
  obs_credible_interval <- quantile(samples, c(0.025, 0.975))
  
  ci_limits[i, ] <- obs_credible_interval
}
```

```{r}
merged_df = df %>% 
  bind_cols(data.frame(CI_L = ci_limits[,1], CI_R = ci_limits[,2])) %>% 
  mutate(Inside_CI = (u >= CI_L & u <= CI_R)) 
merged_df %>% 
  ggplot(aes(x = 1:N_obs, y = u, ymin = CI_L, ymax = CI_R, color=Inside_CI)) +
  geom_point() + 
  geom_errorbar() +
  theme_minimal() +
  labs(x = "Point", y = "Unemployment Rate")
```

Plot 7. 95% credible intervals and predictions based on Model 1.